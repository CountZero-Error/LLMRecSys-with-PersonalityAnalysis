{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f12db5f-f8db-4779-a2bb-858fe22202e0",
   "metadata": {},
   "source": [
    "# LLM for Recommendation System - RAG\n",
    "\n",
    "## TABLE OF CONTENT\n",
    "### $~~~$ - 1. Recommendation System\n",
    "### $~~~$ - 2. Result Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838102f-2bd4-4f3f-bc10-2115dfbe6a30",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b3d38-ee00-49e9-a6de-ee2a2f3a5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python vision\n",
    "!python -V\n",
    "# Check CUDA vision\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3500fbf-bbbb-4e40-8f65-4727a0c49dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import pipeline\n",
    "from time import time \n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be73e3-1784-4091-9074-d17e37915c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU Availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available else \"cpu\")\n",
    "#device = 'cpu' # Set to cpu when debugging\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "access_token = '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'\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = access_token\n",
    "\n",
    "base_dir = \"../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88669f-7054-41ad-8891-4e4850e557d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"[*] Tokenizer loaded.\")\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    token=access_token,\n",
    ").to(device)\n",
    "print(\"[*] Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8462da-334e-42c3-b012-65f29b074910",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "vector_db_dir = os.path.join(base_dir, 'Vector_DB')\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(\n",
    "    vector_db_dir,\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29708b5-6c6e-437a-8eab-67807901effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = pd.read_csv(os.path.join(base_dir, 'trainData/amazon_products.train.formatted.csv'))\n",
    "\n",
    "def retrieve_product_information(df, query_value):\n",
    "    product_index = df.index[df['PRODUCT_ID'] == query_value].tolist()[0]\n",
    "    full_text = formatted_df.loc[product_index, 'TEXT']\n",
    "    print(f'[*] Retrieved product full content:\\n{full_text}')\n",
    "\n",
    "    return formatted_df.loc[product_index, 'DESCRIPTION'], full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835cc2e-cfa0-4039-b15b-6a247061e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rec_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=1000,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494bf6a-84ab-4705-905b-609c128428a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Response should include product id, title, and reason for recommendation.\n",
    "Information of recommended products must be correct, do not falsify information.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "Answer the question in format: <product id>: <title>\\nReason: ...\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4bacf-e443-4bb6-b8e9-1531edf2d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(time())\n",
    "random_product_id = random.choice(formatted_df['PRODUCT_ID'])\n",
    "test_description, full_text = retrieve_product_information(formatted_df, random_product_id)\n",
    "\n",
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=test_description, k=11)[1:] # The first one will always be the qurey one, so skip it.\n",
    "retrieved_docs_text = [\n",
    "    doc.metadata['text'] for doc in retrieved_docs\n",
    "]  # We only need the text of the documents\n",
    "\n",
    "context = \"\\nExtracted products:\"\n",
    "context += \"\".join(\n",
    "    [f\"\\n\\nProduct {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    ")\n",
    "\n",
    "final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
    "    question=\"Base on this product, recommend 5 best products from Context.\", context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6d3f6-c6a3-474a-bf26-aac0e177a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redact an answer\n",
    "recommedations = Rec_LLM(final_prompt)[0][\"generated_text\"]\n",
    "print(recommedations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f18ba5-893f-44fe-9a00-ced6ef9a467d",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Result Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523520f-3e17-40b5-b759-c04b8e0f3078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
