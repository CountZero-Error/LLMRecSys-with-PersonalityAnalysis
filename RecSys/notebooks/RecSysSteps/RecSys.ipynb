{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f12db5f-f8db-4779-a2bb-858fe22202e0",
   "metadata": {},
   "source": [
    "# LLM for Recommendation System - RAG\n",
    "\n",
    "## TABLE OF CONTENT\n",
    "### $~~~$ - 1. Load Tokenizer and Model from HuggingFace\n",
    "### $~~~$ - 2. Load Vector Database\n",
    "### $~~~$ - 3. Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838102f-2bd4-4f3f-bc10-2115dfbe6a30",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Tokenizer and Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b3d38-ee00-49e9-a6de-ee2a2f3a5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python vision\n",
    "!python -V\n",
    "# Check CUDA vision\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3500fbf-bbbb-4e40-8f65-4727a0c49dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from getpass import getpass\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be73e3-1784-4091-9074-d17e37915c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU Availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available else \"cpu\")\n",
    "#device = 'cpu' # Set to cpu when debugging\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be749e-38f5-4ca1-9340-0f446f1cd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ab499-62d8-4d15-a0a3-0bf452076a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = getpass()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec7b99-6c8f-4b33-bd19-ce40af255fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_id = \"Qwen/Qwen2.5-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88669f-7054-41ad-8891-4e4850e557d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"[*] Tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d66327-4c4b-429f-8ce0-d8f362cf9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    token=access_token,\n",
    "    # quantization_config=bnb_config,\n",
    ").to(device)\n",
    "print(\"[*] Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7dc8920201b91",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Embedding Model and Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03c4e9-db67-4fa6-89db-61590626d828",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5a4b9-a002-42ee-a649-12a4a7f7e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a5fce-0755-4759-a3d5-f93bc135e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8462da-334e-42c3-b012-65f29b074910",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390fcca-aa27-4954-8f41-35f45037da8f",
   "metadata": {},
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a321a-49cf-4022-b65f-bec89dcbb93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05888b5-1d5e-4f7a-9e9b-750a6c836334",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef0c17fb39045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_dir = os.path.join(base_dir, 'Vector_DB')\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(\n",
    "    vector_db_dir,\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77787861-884b-4a6c-90a7-4c76be896fd9",
   "metadata": {},
   "source": [
    "### Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e5662-6739-48b8-90c0-c40e831d1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f99971-0d26-49eb-b73f-1ff7a261034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = pd.read_csv(os.path.join(base_dir, 'trainData/amazon_products.train.formatted.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29708b5-6c6e-437a-8eab-67807901effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_product_information(df, query_value):\n",
    "    product_index = df.index[df['PRODUCT_ID'] == query_value].tolist()[0]\n",
    "    full_text = formatted_df.loc[product_index, 'TEXT']\n",
    "    print(f'[*] Retrieved product full content:\\n{full_text}')\n",
    "\n",
    "    return formatted_df.loc[product_index, 'DESCRIPTION'], full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a0b0e-6f7d-4848-8a81-92a6259aaa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(time())\n",
    "random_product_id = random.choice(formatted_df['PRODUCT_ID'])\n",
    "test_description, full_text = retrieve_product_information(formatted_df, random_product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a53d9-0aa9-4563-aae1-a335d7ec4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[*] Starting retrieval for description:\\n{test_description=}\\n\")\n",
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=test_description, k=6)[1:] # The first one will always be the qurey one, so skip it.\n",
    "print(\"==================================Top document==================================\")\n",
    "print(retrieved_docs[0].page_content)\n",
    "print(\"====================================Full Content====================================\")\n",
    "print(retrieved_docs[0].metadata['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd4350a-140a-4d77-956a-27b68efff9d3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16057e17-138a-401c-b8d1-15bba0e6fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835cc2e-cfa0-4039-b15b-6a247061e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rec_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=1000,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2c883-f7ea-4248-9f09-5e62d1c30a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "Q = \"What is 4+4? Answer:\"\n",
    "A = Rec_LLM(Q)\n",
    "print(f'[*] {Q}{A[0]['generated_text']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2be54e-0e4e-4745-a946-e729fe26c0fd",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494bf6a-84ab-4705-905b-609c128428a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in context, give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Information of recommended products must be correct and matched in context, do not falsify information.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "\n",
    "Response must include product id, title, and reason for recommendation.\n",
    "Response must strictly follow the template below:\n",
    "i. - Product ID: <product id>: \n",
    "   - Title: <title>\n",
    "   - Reason: <reason for recommendation>\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37ca2e-12eb-481d-ba42-ac2f96a0229a",
   "metadata": {},
   "source": [
    "### Recommendation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4bacf-e443-4bb6-b8e9-1531edf2d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(time())\n",
    "random_product_id = random.choice(formatted_df['PRODUCT_ID'])\n",
    "test_description, full_text = retrieve_product_information(formatted_df, random_product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acfa4b-18de-406f-8515-b3f5a12fb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=test_description, k=11)[1:] # The first one will always be the qurey one, so skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6d3f6-c6a3-474a-bf26-aac0e177a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs_text = [\n",
    "    doc.metadata['text'] for doc in retrieved_docs\n",
    "]  # We only need the text of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fad6d-074a-423f-8005-fa2cd58d0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\nExtracted products:\"\n",
    "context += \"\".join(\n",
    "    [f\"\\n\\nProduct {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f5d69-2ba8-4ffc-b764-6c0d9c7f0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
    "    question=\"Base on this product, recommend 5 best products from Context.\", context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92cda77-af1e-47a5-b3a4-6c7b5b44890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redact an answer\n",
    "recommedations = Rec_LLM(final_prompt)[0][\"generated_text\"]\n",
    "print(recommedations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db36b13-d716-4c9d-a480-c6ae75b62f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf76ab0-fd78-43c6-8c54-6a00f3e390e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
